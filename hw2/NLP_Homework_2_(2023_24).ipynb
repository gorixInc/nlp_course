{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI3CQO3bD9r4"
      },
      "source": [
        "# Homework 2\n",
        "# Recurrent Neural Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQsRfPGAD6OE"
      },
      "source": [
        "The second homework is comprised of two parts:\n",
        "\n",
        "*   Task A. Named Entity Recognition\n",
        "*   Task B. Summarization\n",
        "\n",
        "To do this homework, make a copy of this notebook (`File` -> `Save a copy in Drive`) and work on it. Alternatively, download the notebook if you want to work on it locally (`File` -> `Download` -> `Download .ipynb`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBN4L14Zp1yQ"
      },
      "source": [
        "**Note:** The model is intended to be trained in GPU, training on CPU will take considerably more time. To use GPU in Colab: `Runtime->Change runtime type-> Choose GPU as a hardware accelerator.`. Be aware that GPU hours in Colab are limited to 3-4 hours, so it is advised to develop the model in CPU (`None` as hardware accelerator) and change the hardware accelerator to GPU when you are ready to train your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_TORvLOwD18z"
      },
      "outputs": [],
      "source": [
        "!pip3 install --quiet torchtext datasets torchinfo torchdata evaluate seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4BKCPtPEzvt"
      },
      "source": [
        "## Task A. Named Entity Recognition [6 Points]\n",
        "\n",
        "In this task, we will work on the Named Entity Recognition(NER) task. We will concentrate on four types of named entities: persons, locations, organizations, and names of miscellaneous entities that do not belong to the previous three groups. As a dataset, we will use [conll2003](https://huggingface.co/datasets/conll2003)  dataset. The dataset has already been uploaded for you. The dataset contains the following columns:\n",
        "* `id`: an identifier.\n",
        "* `tokens`: a list of tokens.\n",
        "* `tags`: a list of classification labels (NER tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-XHSHr_cFomf"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "old_indices= ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "\n",
        "def org_net_tag(data):\n",
        "  ner_tags= [[old_indices[tag] for tag in ner] for ner in data['ner_tags']]\n",
        "  data['tags']=ner_tags\n",
        "  return data\n",
        "dataset = dataset.map(org_net_tag, batched=True,remove_columns=[\"pos_tags\", \"chunk_tags\",\"ner_tags\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Dp9f0884G-bs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9eCaEJz9G-1m"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'tags': ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhgXBHFnFsgX"
      },
      "source": [
        "### Task 1. Preparing the dataset [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZfBB79mHBve"
      },
      "source": [
        "Since the sentences are already tokenized, we will directly start to create vocabulary, process the datasets and create mini-batches of the datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv2PMyUnyJL5"
      },
      "source": [
        "\n",
        "**a. Vocabulary**\n",
        "\n",
        "You need to create 2 vocabularies: one for tokens and one for tags:\n",
        "*   Create tokens vocabulary by using tokens in the training data. Add `<pad>` and `<unk>` special tokens. Make index for `<unk>` token default for the tokens vocabulary.\n",
        "*   Create NER tags vocabulary by using tags in the training data. Add `<pad>` special token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TZtQzozMb1JC"
      },
      "outputs": [],
      "source": [
        "from torchtext.vocab import vocab\n",
        "from collections import Counter\n",
        "\n",
        "token_counter = Counter()\n",
        "tag_counter = Counter()\n",
        "\n",
        "\n",
        "for doc in dataset['train']:\n",
        "    token_counter.update(doc['tokens'])\n",
        "    tag_counter.update(doc['tags']) \n",
        "    \n",
        "token_vocab = vocab(token_counter, specials=('<unk>', '<pad>'))\n",
        "token_vocab.set_default_index(token_vocab['<unk>'])\n",
        "\n",
        "tag_vocab = vocab(tag_counter, specials=('<pad>',))\n",
        "#tag_vocab.set_default_index(tag_vocab['<unk>'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyzBxwPsyMk7"
      },
      "source": [
        "**b. Process the datasets**\n",
        "\n",
        "For each dataset split:\n",
        "*   Convert tokens of the dataset into token ids by using tokens vocabulary.\n",
        "*   Convert tags of the dataset into tag ids by using NER tags vocabulary\n",
        "*   Convert final results (both token ids and tag ids) into Pytorch tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RRqymjpTyUa7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def convert_to_ids(sentences, vocab):\n",
        "    return [torch.tensor(vocab.lookup_indices(sentence)) for sentence in sentences]\n",
        "\n",
        "\n",
        "tensor_dataset = {}\n",
        "for split in dataset:\n",
        "    split_dataset = dataset[split]\n",
        "    token_ids = convert_to_ids(split_dataset['tokens'], token_vocab)\n",
        "\n",
        "    tag_ids = convert_to_ids(split_dataset['tags'], tag_vocab)\n",
        "\n",
        "    tensor_dataset[split] = {}\n",
        "    tensor_dataset[split]['token_tensors'] = token_ids\n",
        "    tensor_dataset[split]['tag_tensors'] = tag_ids\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "11saw93cyUQ8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUTvuFwayUwV"
      },
      "source": [
        "\n",
        "**c. Automatic batching**\n",
        "\n",
        "In order to automatically batch each dataset split:\n",
        "*   Use `DataLoader` for automatic batching\n",
        "*   In collate function, return\n",
        "    *   padded tokens\n",
        "    *   padded tags\n",
        "    *   length of sentences\n",
        "    *   *Note: If you will use GPU for training, send the outputs of collate function to GPU (device).*\n",
        "*   Decide on batch size (e.g. 8, 16, 32, or any 2^n) and initialize DataLoader for each dataset split. If you run out of memory, change the batch size to a lower value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xhYJBeWfyVjd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, token_vocab, tag_vocab):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.token_vocab = token_vocab\n",
        "        self.tag_vocab = tag_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sentences[idx], self.tags[idx]\n",
        "    \n",
        "def collate_fn(batch):\n",
        "    # Separate the tokens and tags and sort by length (descending)\n",
        "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    tokens, tags = zip(*batch)\n",
        "    \n",
        "    # Get lengths of each sequence before padding\n",
        "    lengths = [len(seq) for seq in tokens]\n",
        "\n",
        "    # Pad the sequences\n",
        "    tokens_padded = pad_sequence(tokens, batch_first=True, padding_value=token_vocab['<pad>'])\n",
        "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=tag_vocab['<pad>'])\n",
        "    \n",
        "    tokens_padded.to(device)\n",
        "    tags_padded.to(device)\n",
        "\n",
        "    return tokens_padded, tags_padded, torch.tensor(lengths)    \n",
        "\n",
        "ner_dataset_train = NERDataset(tensor_dataset['train']['token_tensors'], \n",
        "                         tensor_dataset['train']['tag_tensors'], \n",
        "                         token_vocab, tag_vocab)\n",
        "ner_dataset_valid = NERDataset(tensor_dataset['validation']['token_tensors'], \n",
        "                         tensor_dataset['validation']['tag_tensors'], \n",
        "                         token_vocab, tag_vocab)\n",
        "ner_dataset_test = NERDataset(tensor_dataset['test']['token_tensors'], \n",
        "                         tensor_dataset['test']['tag_tensors'], \n",
        "                         token_vocab, tag_vocab)\n",
        "\n",
        "ner_dataloader_train = DataLoader(ner_dataset_train, batch_size=64, collate_fn=collate_fn)\n",
        "ner_dataloader_valid = DataLoader(ner_dataset_valid, batch_size=64, collate_fn=collate_fn)\n",
        "ner_dataloader_test = DataLoader(ner_dataset_test, batch_size=64, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wgN41gSIyV-0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsL1TjQcLV4d"
      },
      "source": [
        "### Task 2. RNN model [1.5 point]\n",
        "\n",
        "Here, you will need to create an RNN model class. It will take the tokens and lengths of sentences as input and will produce NER tag ids for each token in the sentence.\n",
        "\n",
        "In the model, define  \n",
        "*  **Embedding layer** with the number of embeddings equal to\n",
        "size of tokens vocabulary and embedding size to `emb_dim`. Also, specify `padding_idx=0`.\n",
        "*   **Bidirectional LSTM layer**. It should have the hidden size equal to `hid_dim` and the number of layers equal to `n_layers`. Also, specify `batch_first=True`.\n",
        "*   **Classification layer.** Decide the structure of the classification layer. You can use a single Linear layer, or use multiple Linear layers, ReLU, and/or Dropout between. If you will use several layers for classification, you can store all layers in `nn.Sequential` for easy use in the forward method. The input size for the classification layer should be double the size of `hid_dim` because we are using bidirectional LSTM and the output size should be the size of the NER tags vocabulary.\n",
        "*   **Dropout layer** with the dropout probability of `0.5`.\n",
        "\n",
        "In the forward method:\n",
        "\n",
        "*   Encode the input `tokens` with the embedding layer.\n",
        "*   Apply dropout on the embeddings.\n",
        "*   Pack the embedded inputs with [pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html) function. Make sure to specify `batch_first=True` and `enforce_sorted=False`.\n",
        "*   Pass the packed inputs to the LSTM layer.\n",
        "*   Get the output features from the last layer of the LSTM and pass it to [pad_packed_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html). Make sure to specify `batch_first=True`. *Note: `pad_packed_sequence` is inverse operation to `pack_padded_sequence`.*\n",
        "*   Pass padded sequence from `pad_packed_sequence` to classification layer\n",
        "*   Return prediction from the classification layer\n",
        "\n",
        "*Note: Refer to the [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM) documentation to get more information about the parameters, the inputs, and outputs.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mTTczbpcjEy8"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class BiLSTMNERTagger(nn.Module):\n",
        "    def __init__(self, emb_dim, hid_dim, n_layers, token_vocab_size, tag_vocab_size):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(token_vocab_size, emb_dim, padding_idx=1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.lstm_bidirectional = nn.LSTM(input_size=emb_dim, num_layers=n_layers, \n",
        "                                          batch_first=True, hidden_size=hid_dim, \n",
        "                                          bidirectional=True)\n",
        "        self.classification = nn.Sequential(\n",
        "           nn.Linear(hid_dim*2, hid_dim*2),\n",
        "           nn.ReLU(),\n",
        "           nn.Linear(hid_dim*2, tag_vocab_size)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, words, words_len):\n",
        "      emb = self.emb(words)\n",
        "      emb = self.dropout(emb)\n",
        "      emb = pack_padded_sequence(emb, batch_first=True, enforce_sorted=False, lengths=words_len)\n",
        "      lstm_output, (hn, cn) = self.lstm_bidirectional(emb)\n",
        "      seq_unpacked, lens_unpacked = pad_packed_sequence(lstm_output, batch_first=True, padding_value=1)\n",
        "      prediction = self.classification(seq_unpacked)\n",
        "    \n",
        "      return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEsQTaUQLgqr"
      },
      "source": [
        "### Task 3. Training [2 points]\n",
        "**a. Initialize the model:**\n",
        "\n",
        "*   Calculate token and tag vocabulary sizes\n",
        "*   Decide on the number of layers (`n_layers`), sizes of embedding(`emb_dim`), and hidden dimensions (`hid_dim`).\n",
        "*   Initialize the model by passing arguments to the model. Send the model to GPU if you are using one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KC3qkvDQ4dnN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BiLSTMNERTagger(\n",
              "  (emb): Embedding(23625, 64, padding_idx=1)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (lstm_bidirectional): LSTM(64, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
              "  (classification): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_vocab_size = len(token_vocab)\n",
        "tag_vocab_size = len(tag_vocab)\n",
        "\n",
        "model = BiLSTMNERTagger(64, 128, 3, token_vocab_size, tag_vocab_size)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGzZyIW94RYz"
      },
      "source": [
        "**b.**\n",
        "\n",
        "**Loss function(criterion)**: use [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropy#torch.nn.CrossEntropyLoss) and set `ignore_index` parameter to ignore the padding.\n",
        "\n",
        "**Optimizer**: use [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EbHwiRIU4Q2k"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tag_vocab['<pad>'])\n",
        "optimizer = Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gwaAbpD4TQi"
      },
      "source": [
        "\n",
        "**c. Training:**\n",
        "\n",
        "*   Decide on the number of epochs you want to train.\n",
        "  *  *Note: You can estimate the total time of the training loop by running the training loop for one epoch and multiplying the time with the chosen number of epochs.*\n",
        "  *  *Note: One epoch should not take more than 2-3 minutes on a GPU.*\n",
        "*   Implement the training loop\n",
        "   *   Report training and validation loss for each epoch.\n",
        "   *   If validation loss is decreased, save the model. (You can use `torch.save(model, file_path)` )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fm39y3VB5IFX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Training Loss: 191.80757600069046, Validation Loss: 37.187063217163086\n",
            "Validation loss decreased (inf --> 37.187063). Saving model ...\n",
            "Epoch 2/20, Training Loss: 145.73386059701443, Validation Loss: 26.692867010831833\n",
            "Validation loss decreased (37.187063 --> 26.692867). Saving model ...\n",
            "Epoch 3/20, Training Loss: 110.90190483257174, Validation Loss: 21.525157721713185\n",
            "Validation loss decreased (26.692867 --> 21.525158). Saving model ...\n",
            "Epoch 4/20, Training Loss: 93.20787839964032, Validation Loss: 19.425325771793723\n",
            "Validation loss decreased (21.525158 --> 19.425326). Saving model ...\n",
            "Epoch 5/20, Training Loss: 81.80387690383941, Validation Loss: 18.032829579897225\n",
            "Validation loss decreased (19.425326 --> 18.032830). Saving model ...\n",
            "Epoch 6/20, Training Loss: 73.33292641118169, Validation Loss: 16.31408913107589\n",
            "Validation loss decreased (18.032830 --> 16.314089). Saving model ...\n",
            "Epoch 7/20, Training Loss: 66.04235683009028, Validation Loss: 14.894242275040597\n",
            "Validation loss decreased (16.314089 --> 14.894242). Saving model ...\n",
            "Epoch 8/20, Training Loss: 60.610391698777676, Validation Loss: 13.937395866494626\n",
            "Validation loss decreased (14.894242 --> 13.937396). Saving model ...\n",
            "Epoch 9/20, Training Loss: 55.168941555544734, Validation Loss: 14.029916238505393\n",
            "Epoch 10/20, Training Loss: 51.00821575289592, Validation Loss: 12.834816980175674\n",
            "Validation loss decreased (13.937396 --> 12.834817). Saving model ...\n",
            "Epoch 11/20, Training Loss: 47.287255288567394, Validation Loss: 12.154918428044766\n",
            "Validation loss decreased (12.834817 --> 12.154918). Saving model ...\n",
            "Epoch 12/20, Training Loss: 43.821208093781024, Validation Loss: 11.367588902823627\n",
            "Validation loss decreased (12.154918 --> 11.367589). Saving model ...\n",
            "Epoch 13/20, Training Loss: 41.58104063454084, Validation Loss: 10.970168196363375\n",
            "Validation loss decreased (11.367589 --> 10.970168). Saving model ...\n",
            "Epoch 14/20, Training Loss: 39.079971560742706, Validation Loss: 10.993711684714071\n",
            "Epoch 15/20, Training Loss: 36.63077634072397, Validation Loss: 11.00999323441647\n",
            "Epoch 16/20, Training Loss: 34.23972002672963, Validation Loss: 10.35213470319286\n",
            "Validation loss decreased (10.970168 --> 10.352135). Saving model ...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Forward pass: Get logits from the model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Compute the loss and perform a backward pass\u001b[39;00m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), tags_padded\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
            "File \u001b[1;32mc:\\Users\\Gordei\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Gordei\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[14], line 23\u001b[0m, in \u001b[0;36mBiLSTMNERTagger.forward\u001b[1;34m(self, words, words_len)\u001b[0m\n\u001b[0;32m     21\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(emb)\n\u001b[0;32m     22\u001b[0m emb \u001b[38;5;241m=\u001b[39m pack_padded_sequence(emb, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lengths\u001b[38;5;241m=\u001b[39mwords_len)\n\u001b[1;32m---> 23\u001b[0m lstm_output, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_bidirectional\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m seq_unpacked, lens_unpacked \u001b[38;5;241m=\u001b[39m pad_packed_sequence(lstm_output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification(seq_unpacked)\n",
            "File \u001b[1;32mc:\\Users\\Gordei\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Gordei\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "num_epochs = 20\n",
        "best_val_loss = np.inf\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for tokens_padded, tags_padded, lengths in ner_dataloader_train:\n",
        "        tokens_padded = tokens_padded.to(device)\n",
        "        tags_padded = tags_padded.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass: Get logits from the model\n",
        "        logits = model(tokens_padded, lengths)\n",
        "\n",
        "        # Compute the loss and perform a backward pass\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), tags_padded.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    for tokens_padded, tags_padded, lengths in ner_dataloader_valid:\n",
        "        tokens_padded = tokens_padded.to(device)\n",
        "        tags_padded = tags_padded.to(device)\n",
        "\n",
        "        outputs = model(tokens_padded, lengths)\n",
        "        loss = criterion(outputs.view(-1, tag_vocab_size), tags_padded.view(-1))\n",
        "        total_val_loss += loss.item()\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {total_train_loss}, Validation Loss: {total_val_loss}')\n",
        "\n",
        "    if total_val_loss < best_val_loss:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model ...'.format(\n",
        "            best_val_loss,\n",
        "            total_val_loss))\n",
        "        torch.save(model.state_dict(), 'model.pth')\n",
        "        best_val_loss = total_val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riRyTdMgLjY7"
      },
      "source": [
        "### Task 4. Evaluation [1.5 points]\n",
        "\n",
        "To evaluate the validation and test datasets, we will use [seqeval](https://huggingface.co/spaces/evaluate-metric/seqeval) from HuggingFace.\n",
        "It takes two mandatory arguments:\n",
        "*  predictions: a list of lists of predicted labels\n",
        "*  references: a list of lists of reference labels\n",
        "\n",
        "In order to pass these arguments to the model, first you should get the model's prediction on the datasets and prepare predictions for the seqeval.\n",
        "\n",
        "**a.** First, for evaluation on **validation** set:\n",
        "*  Load the saved model with the lowest validation loss and set it to evaluation mode. *Note: You can use `torch.load('path_to_model')` to load the model*\n",
        "*  Loop over data loader\n",
        "*  Get predictions from the model and convert model prediction to NER ids by using [argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html). Specify `dim=2`.\n",
        "*  Remove paddings from the predictions and references (true labels) using the length of the sentence\n",
        "* Convert predictions and references NER ids to NER tags by using NER tag vocabulary (`lookup_tokens` function)\n",
        "* Add predictions and references to the separate lists\n",
        "* Pass these lists to `seqeval` and print results.\n",
        "* *Note: Your model should achieve a minimum 0.60 `overall_f1` score on the validation dataset. If your model has a lower overall F1 score, you should consider going back to the previous task and trying to change the parameters of the model or training.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlfxoBJwjvpz"
      },
      "outputs": [],
      "source": [
        "model = BiLSTMNERTagger(64, 128, 3, token_vocab_size, tag_vocab_size)\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model.eval()\n",
        "total_val_loss = 0\n",
        "\n",
        "def remove_padding(seqs, lengths):\n",
        "    new_seqs = []\n",
        "    for i, seq in enumerate(seqs):\n",
        "        new_seq = seq[:lengths[i]]\n",
        "        new_seqs.append(new_seq)\n",
        "    return new_seqs\n",
        "\n",
        "def seqs_to_tok(seqs, vocab):\n",
        "    seqs_tokens = []\n",
        "    for seq in seqs:\n",
        "        seqs_tokens.append(vocab.lookup_tokens(list(seq)))\n",
        "    return seqs_tokens\n",
        "\n",
        "\n",
        "def get_all_preds(dataloader):\n",
        "    all_preds, all_tags = [], []\n",
        "    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
        "        for tokens_padded, tags_padded, lengths in dataloader:\n",
        "            tokens_padded = tokens_padded.to(device)\n",
        "            tags_padded = tags_padded.to(device)\n",
        "\n",
        "            outputs = model(tokens_padded, lengths)\n",
        "            preds = torch.argmax(outputs, dim=2)\n",
        "\n",
        "            preds_unpad = remove_padding(preds, lengths)\n",
        "            tags_unpad = remove_padding(tags_padded, lengths)\n",
        "\n",
        "            preds_tokens = seqs_to_tok(preds_unpad, tag_vocab)\n",
        "            tags_tokens = seqs_to_tok(tags_unpad, tag_vocab)\n",
        "\n",
        "            all_preds += preds_tokens\n",
        "            all_tags += tags_tokens\n",
        "    return all_preds, all_tags\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEbx14y_LgZX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7773584905660378\n"
          ]
        }
      ],
      "source": [
        "from seqeval.metrics import accuracy_score\n",
        "from seqeval.metrics import classification_report\n",
        "from seqeval.metrics import f1_score\n",
        "\n",
        "all_tags, all_preds = get_all_preds(ner_dataloader_valid)\n",
        "print(f1_score(all_tags, all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WuNiy4AGixf"
      },
      "source": [
        "**b. Evaluation of the test set.** After you achieve the desired result on the validation set, use similar procedures as above to evaluate the model on the test set and print the results of `seqeval`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IsbMkoIKO9E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7215311004784689\n"
          ]
        }
      ],
      "source": [
        "all_tags, all_preds = get_all_preds(ner_dataloader_test)\n",
        "print(f1_score(all_tags, all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRAl01PVJCbR"
      },
      "source": [
        "## Task B. Summarization [9 Points]\n",
        "\n",
        "Summarization is the task of condensing a piece of text to a shorter version that contains the main information from the original. For this task, you will implement a sequence-to-sequence attentional model with a simple version of Pointer-Generator Networks introduced in this [paper](https://arxiv.org/abs/1704.04368).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Note:** In the modeling part of this task, we are providing a starter code. You are free to change any part of the code if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYPO8KrYNFR9"
      },
      "source": [
        "For this task,  we will be using [SAMSum](https://huggingface.co/datasets/samsum) dataset. The SAMSum dataset contains about 16k messenger-like conversations with summaries.\n",
        "\n",
        "The dataset contains following data fields:\n",
        "* `dialogue`: text of dialogue.\n",
        "* `summary`: human written summary of the dialogue.\n",
        "* id: unique id of an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zohLNhVcNWqc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Gordei\\miniconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 14732\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 819\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 818\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"samsum\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MWJSWnTPPYEh"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '13818513',\n",
              " 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
              " 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1JvhZeTPgIr"
      },
      "source": [
        "### Task 1. Dataset processing [1 point]\n",
        "\n",
        "**a. Vocabulary**\n",
        "\n",
        "You need to create a vocabulary:\n",
        "*   Create a vocabulary by using tokens in `dialogue` and `summary` fields of the training data.\n",
        "*   Add `<pad>`, `<unk>` , `<sos>`, and `<eos> `special tokens.\n",
        "*   Make index for `<unk>` token default for the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qRjvMBy9QB3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32526\n"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import vocab\n",
        "from collections import Counter\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import re\n",
        "\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "#tokenizer = get_tokenizer('basic_english')\n",
        "joint_counter = Counter()\n",
        "\n",
        "\n",
        "def remove_emojis(s):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
        "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
        "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
        "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
        "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
        "                               u\"\\U000024C2-\\U0001F251\" \n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', s)\n",
        "\n",
        "def keep_alphanumeric(s):\n",
        "    pattern = re.compile(r'[^a-zA-Z0-9\\s.,;:!?-]+')  # Pattern to match non-alphanumeric characters\n",
        "    return pattern.sub('', s)\n",
        "\n",
        "def preprocessing(s):\n",
        "    s = s.replace('\\r\\n', ' ')\n",
        "    s = s.replace('\\n', ' ')\n",
        "    s = s.replace('\\r', ' ')\n",
        "    s = remove_emojis(s)\n",
        "    s = s.lower()\n",
        "    s = keep_alphanumeric(s)\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "\n",
        "for doc in dataset['train']:\n",
        "    #print(repr(preprocessing(doc['dialogue'])))\n",
        "    tokenized_dialogue = tokenizer(preprocessing(doc['dialogue']))\n",
        "    tokenized_summary = tokenizer(preprocessing(doc['summary']))\n",
        "    \n",
        "    joint_counter.update(tokenized_dialogue)\n",
        "    joint_counter.update(tokenized_summary)\n",
        "\n",
        "joint_vocab = vocab(joint_counter, specials=('<unk>', '<pad>', '<sos>', '<eos>'))\n",
        "joint_vocab.set_default_index(joint_counter['<unk>'])\n",
        "\n",
        "print(len(joint_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW62hPgMQIVO"
      },
      "source": [
        "**b. Process the datasets**\n",
        "\n",
        "For each dataset split:\n",
        "*   Convert the dataset into token IDs by using created vocabulary\n",
        "*   Calculate source (`dialogue`) and target(`summary`) lengths of each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mvHOb4oUQH--"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def convert_to_ids(sentences, vocab, tokenizer):\n",
        "    tensors = []\n",
        "    for sentence in sentences:\n",
        "        sentence = preprocessing(sentence)\n",
        "        tokensized_sentence = tokenizer(sentence)\n",
        "        if len(tokensized_sentence) <= 0:\n",
        "            continue\n",
        "        tensor = torch.tensor(vocab.lookup_indices(tokensized_sentence))\n",
        "        tensors.append(tensor)\n",
        "    return tensors\n",
        "\n",
        "\n",
        "tensor_dataset = {}\n",
        "for split in dataset:\n",
        "    split_dataset = dataset[split]\n",
        "\n",
        "    dialogue_ids = convert_to_ids(split_dataset['dialogue'], joint_vocab, tokenizer)\n",
        "    summary_ids = convert_to_ids(split_dataset['summary'], joint_vocab, tokenizer)\n",
        "    \n",
        "    dialogue_lens = torch.tensor([len(dialogue) for dialogue in dialogue_ids])\n",
        "    summary_lens = torch.tensor([len(summary) for summary in summary_ids])\n",
        "\n",
        "    tensor_dataset[split] = {}\n",
        "    \n",
        "    tensor_dataset[split]['dialogue_ids'] = dialogue_ids\n",
        "    tensor_dataset[split]['summary_ids'] = summary_ids\n",
        "\n",
        "    tensor_dataset[split]['dialogue_lens'] = dialogue_lens\n",
        "    tensor_dataset[split]['summary_lens'] = summary_lens\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya9AoJhHRJ1v"
      },
      "source": [
        "\n",
        "**c. Automatic batching**\n",
        "\n",
        "*   Calculate maximum length of the sources (SOURCE_LEN) and targets (TARGET_LEN)\n",
        "*   Pad each source and target to their maximum length (SOURCE_LEN and TARGET_LEN, respectively) and return appropriate fields with collate function\n",
        "*   Decide on batch size (e.g., 8, 16, 32, or any 2^n) and initialize DataLoader for each dataset split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LO_JjBiUS_Vv"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "PAD_IDX = joint_vocab['<pad>']\n",
        "SOS_IDX = joint_vocab['<sos>']\n",
        "EOS_IDX = joint_vocab['<eos>']\n",
        "# Not using SOURCE LEN as it is not needed, padding per batch for better performance\n",
        "#SOURCE_LEN = max(tensor_dataset['train']['dialogue_lens']) + 1  # FOR EOS\n",
        "TARGET_LEN = max(tensor_dataset['train']['summary_lens']) + 1  # FOR EOS\n",
        "\n",
        "class SEMSUMDataset(Dataset):\n",
        "    def __init__(self, tensor_dataset):\n",
        "        self.dialogules = tensor_dataset['dialogue_ids']\n",
        "        self.summaries = tensor_dataset['summary_ids']\n",
        "\n",
        "        self.dialogue_lens = tensor_dataset['dialogue_lens']\n",
        "        self.summary_lens = tensor_dataset['summary_lens']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dialogules)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dialogules[idx], self.summaries[idx], self.dialogue_lens[idx], self.summary_lens[idx]\n",
        "    \n",
        "def collate_fn(batch):\n",
        "    #batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
        "    #source, target, source_len, target_len = zip(*batch)\n",
        "    \n",
        "    source_batch, target_batch, source_lens, target_lens = [], [], [], []\n",
        "    for source, target, source_len, target_len in batch:\n",
        "        source_batch.append(torch.cat([source, torch.tensor([EOS_IDX])], dim=0))\n",
        "        target_batch.append(torch.cat([target, torch.tensor([EOS_IDX])], dim=0))\n",
        "        source_lens.append(source_len + 1)\n",
        "        target_lens.append(target_len + 1)                         \n",
        "    #source_batch[0] = nn.ConstantPad1d((0, SOURCE_LEN - source_batch[0].shape[0]), PAD_IDX)(source_batch[0])\n",
        "    #target_batch[0] = nn.ConstantPad1d((0, TARGET_LEN - target_batch[0].shape[0]), PAD_IDX)(target_batch[0])\n",
        "\n",
        "    # Pad the sequences, padding both target and source to max in batch\n",
        "    source_padded = pad_sequence(source_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    target_padded = pad_sequence(target_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    \n",
        "    return source_padded, target_padded, torch.tensor(source_lens), torch.tensor(target_lens) \n",
        "\n",
        "semsum_dataset_train = SEMSUMDataset(tensor_dataset['train'])\n",
        "semsum_dataset_valid = SEMSUMDataset(tensor_dataset['validation'])\n",
        "semsum_dataset_test = SEMSUMDataset(tensor_dataset['test'])\n",
        "\n",
        "semsum_dataloader_train = DataLoader(semsum_dataset_train, batch_size=64, collate_fn=collate_fn, shuffle=True)\n",
        "semsum_dataloader_valid = DataLoader(semsum_dataset_valid, batch_size=64, collate_fn=collate_fn)\n",
        "semsum_dataloader_test = DataLoader(semsum_dataset_test, batch_size=64, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIPQQhJGTDkS"
      },
      "source": [
        "### Task 2. Encoder [1 point]\n",
        "\n",
        "In the encoder, we will use a single-layer bidirectional LSTM. Additionally, we will define two linear layers to reduce the dimensionality of the concatenated hidden (`h_n`) and cell states (`c_n`). These layers will take the concatenation of the forward and backward hidden and cell states from the bidirectional LSTM as input and produce hidden states of dimensionality `hidden_size`.\n",
        "  \n",
        "**Forward Method:**\n",
        "1. Pass the input sequences (`input`) through the bidirectional LSTM layer.\n",
        "2. Concatenate the forward and backward hidden states along the last dimension.\n",
        "3. Apply linear transformations to reduce the dimensionality of the concatenated hidden and cell states using the defined linear layers\n",
        "4. Apply the ReLU activation function to the outputs of the linear layers.\n",
        "5. Return the output sequence of the LSTM along with the final hidden and cell states.\n",
        "\n",
        "**Additional Notes:**\n",
        "- Ensure that the LSTM layer is configured to handle batched input sequences correctly\n",
        "- Use PyTorch functions such as `torch.cat` and `unsqueeze` for tensor manipulations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AHBXw70_TLWT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        #TODO: define a single-layer bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, 1, batch_first=True, bidirectional=True)\n",
        "\n",
        "        #TODO: Define two linear layers with bias to reduce the dimensionality\n",
        "             # of the concatenated hidden (h_n) and cell states (c_n) to `hidden_size`\n",
        "        self.reduce_h_n = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.reduce_c_n = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "    def forward(self, input, input_len):\n",
        "        #print('-----ENCODER------')\n",
        "        #TODO: pass the input to LSTM layer. If you want, you can use\n",
        "             # pack_padded_sequence and pad_packed_sequence for input and output\n",
        "        input_len = input_len.to(torch.int64).to('cpu')\n",
        "        packed_input = nn.utils.rnn.pack_padded_sequence(input, lengths=input_len, batch_first=True, enforce_sorted=False)\n",
        "        #print('input', input.shape)\n",
        "        output, (h_n, c_n) = self.lstm(packed_input)\n",
        "\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
        "        #print('Encoder hidden', h_n.shape)\n",
        "        #print('Encoder cell', c_n.shape)\n",
        "        #TODO: Concatenate the forward and backward hidden and cell states\n",
        "        #h_n = torch.cat((h_n[1], h_n[2]), dim=-1)  \n",
        "        #c_n = torch.cat((c_n[1], c_n[2]), dim=-1)  \n",
        "        #print('---')\n",
        "        h_n = h_n.reshape(1, h_n.size(1), -1)\n",
        "        c_n = c_n.reshape(1, c_n.size(1), -1)\n",
        "        #print('Encoder hidden cat', h_n.shape)\n",
        "        #print('Encoder cell cat', c_n.shape)\n",
        "        #print('---')\n",
        "        #TODO:Apply linear transformations to reduce the dimensionality\n",
        "        h_n = self.reduce_h_n(h_n)\n",
        "        c_n = self.reduce_c_n(c_n)\n",
        "        #print('Encoder hidden cat reduce', h_n.shape)\n",
        "        #print('Encoder cell cat reduce', c_n.shape)\n",
        "        #TODO:Apply the ReLU activation function to the outputs of the linear layers\n",
        "        h_n = F.relu(h_n)\n",
        "        c_n = F.relu(c_n)\n",
        "        #print('--------------------------')\n",
        "        return output, (h_n, c_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6o6-vwjWGIr"
      },
      "source": [
        "### Task 3. Attention [2 points]\n",
        "\n",
        "In the Attention model class, we will define linear projections (`Ea` and `Da`) to transform the encoder outputs and decoder input, respectively, to a common feature space. We will also define a linear layer (`Va`) to learn the importance of each encoder output when computing attention scores.\n",
        "\n",
        "**Forward Method:**\n",
        "  - Compute the attention scores using the Bahdanau attention mechanism (Equations 1 in the [paper](https://arxiv.org/pdf/1704.04368.pdf#page=2)). This involves:\n",
        "    - Projecting the encoder outputs (`encoder_outputs`) and decoder input (`input`) to a common feature space.\n",
        "    - Calculating the attention scores by adding the projected encoder outputs and decoder input and applying the tanh activation function followed by the linear layer (`Va`).\n",
        "  - Compute the attention distribution by applying the softmax function to the masked attention scores (Equations 2 in the [paper](https://arxiv.org/pdf/1704.04368.pdf#page=2))\n",
        "  - Compute the context vector by performing a weighted sum of the encoder outputs using the attention distribution (Equations 3 in the [paper](https://arxiv.org/pdf/1704.04368.pdf#page=3))\n",
        "  - Return the attention distribution and the context vector.\n",
        "\n",
        "**Additional Notes:**\n",
        "- Ensure compatibility of dimensions throughout the operations, especially when performing tensor manipulations such as squeezing, unsqueezing, and summing.\n",
        "- Use PyTorch functions such as `torch.bmm`, `torch.tanh`, `torch.softmax` operations for efficient implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oWMkhGhoWM4d"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        #TODO: define linear layer WITHOUT bias to transform encoder output\n",
        "        self.Ea = nn.Linear(hidden_size*2, hidden_size, bias=False)\n",
        "\n",
        "        #TODO: define linear layer WITH bias to transform decoder input\n",
        "        self.Da = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        #TODO: define linear layer WITHOUT bias to learn attention weights\n",
        "        self.Va = nn.Linear(hidden_size, 1, bias=False)\n",
        "        self.softmax = nn.Softmax(-1)\n",
        "\n",
        "    def forward(self, input, encoder_outputs, pad_mask):\n",
        "        #print('-----ATTENTION------')\n",
        "        #print('Decoder hidden', input.shape)\n",
        "        #print('Encoder output', encoder_outputs.shape)\n",
        "        #TODO: apply linear layer to transform encoder outputs\n",
        "        enc = self.Ea(encoder_outputs) \n",
        "        #print('Linear (encoder)', enc.shape)\n",
        "        #TODO: apply linear layer to transform decoder input\n",
        "\n",
        "        dec = self.Da(input).squeeze(0)  \n",
        "        #print('Linear (decoder)', dec.shape)\n",
        "\n",
        "        #print('-----------------')\n",
        "\n",
        "        #TODO: add transformed encoder and decoder features together to get attention scores\n",
        "        scores = enc + dec.unsqueeze(1)\n",
        "\n",
        "        #TODO: apply the tanh activation function to attention scores\n",
        "        scores = F.tanh(scores)\n",
        "\n",
        "        #TODO: apply the linear layer to attention scores\n",
        "        scores = self.Va(scores).squeeze(2) \n",
        "        if pad_mask is not None:    \n",
        "            scores = scores.float().masked_fill_(\n",
        "                pad_mask,\n",
        "                float(-1e9)\n",
        "            ).type_as(scores)\n",
        "\n",
        "        #TODO:Compute the attention weights by applying the softmax\n",
        "        attn_dist = self.softmax(scores)\n",
        "\n",
        "        #Perform a batch matrix-matrix product of matrices stored in attn_dist and encoder_outputs\n",
        "\n",
        "        context = torch.bmm(attn_dist.unsqueeze(1), encoder_outputs)\n",
        "\n",
        "        context = torch.sum(context, dim=1)\n",
        "\n",
        "        return attn_dist, context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aMUHTfEfeup"
      },
      "source": [
        "### Task 4. Decoder with Attention [1 point]\n",
        "\n",
        "The decoder will generate output sequences based on the attention mechanism, which allows it to focus on relevant parts of the input sequence.\n",
        "We will define a single-layer unidirectional LSTM and use our `Attention` module to compute attention scores and context vectors. Additionally, we will define two linear layers to produce the vocabulary distribution.\n",
        "\n",
        "**Forward Method:**\n",
        "  - Pass the decoder input (`input`) through the LSTM decoder to get the output sequence, hidden and cell states (`h_n` and `c_n`).\n",
        "  - Compute the attention distribution and context vector using the `Attention` module, passing the decoder hidden state, encoder outputs, and padding mask.\n",
        "  - Produce vocabulary distribution (Equation 4 in the [paper](https://arxiv.org/pdf/1704.04368.pdf#page=3)):\n",
        "    - Concatenate the decoder hidden state and context vector\n",
        "    - Pass the concatenated vector through the linear layers `linear_v` and `linear_v_out` to obtain the vocabulary distribution over the output tokens.\n",
        "    - Apply the softmax function to obtain the final probability distribution over the vocabulary.\n",
        "  - Return the vocabulary distribution, attention distribution, context vector, and updated hidden and cell states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r8XR4Sc5fhtk"
      },
      "outputs": [],
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: define a single-layer unidirectional LSTM\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, 1, batch_first=True)\n",
        "\n",
        "        # TODO: define an attention layer\n",
        "        self.attention = Attention(hidden_size)\n",
        "\n",
        "        self.linear_v = nn.Linear(hidden_size * 3, hidden_size, bias=True)\n",
        "        self.linear_v_out = nn.Linear(hidden_size, vocab_size, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input, prev_h, prev_c, encoder_outputs, pad_mask):\n",
        "        #TODO: pass the decoder input, previous hidden and cells to LSTM layer\n",
        "        #print('----DECODER----')\n",
        "        #print('Input', input.shape)\n",
        "        #print('prev_h', prev_h.shape)\n",
        "        #print('prev_c', prev_c.shape)\n",
        "        output, (h_n, c_n) = self.lstm(input, (prev_h, prev_c))\n",
        "        #TODO: pass the hidden states (h_n), encoder_outputs and pad_mask to Attention layer\n",
        "\n",
        "        attn_dist, context = self.attention(h_n, encoder_outputs, pad_mask)\n",
        "\n",
        "        #TODO: concatenate context vector with hidden state of decoder\n",
        "\n",
        "        output =  torch.cat((context, h_n.squeeze(0)), dim=1)\n",
        "\n",
        "        #TOTO:  fed the ouput through two linear layers\n",
        "        output = self.linear_v(output)\n",
        "        output = self.linear_v_out(output)\n",
        "        #TODO: apply softmax to output\n",
        "        vocab_dist = self.softmax(output)\n",
        "        return vocab_dist, attn_dist, context, h_n, c_n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8IFzA-RkRaH"
      },
      "source": [
        "### Task 5. Pointer-Generator Network (2 points)\n",
        "\n",
        "In the PGNet class, we will define the embedding layer, encoder, decoder with attention and add a pointer-generator network (3 linear transformations). The pointer-generator network can copy words from the source text via pointing, which aids accurate reproduction of information while retaining the ability to produce novel words through the generator.\n",
        "\n",
        "**Forward Method:**\n",
        "  - Iterate over the target sequence length (`TARGET_LEN`).\n",
        "  - For each time step `t`:\n",
        "    - Compute the decoder input embedding (`input`) based on the previous decoder output.\n",
        "    - Use the `AttentionDecoder` to generate the vocabulary distribution (`vocab_dist`), attention distribution (`attn_dist`),  context vector, decoder hidden and cell states.\n",
        "    - Calculate the generation probability (`p_gen`) based on the context vector, decoder hidden state, and decoder input (Equation 9 in the [ paper](https://arxiv.org/pdf/1704.04368.pdf#page=3)).\n",
        "    - Compute the final distribution (`final_dist`) by combining the vocabulary distribution and the weighted attention distribution (`w_attn_dist`) based on the generation probability (Equation 9 in the [paper](https://arxiv.org/pdf/1704.04368.pdf#page=3)).\n",
        "  - Stack the final distributions and attention distributions over time steps.\n",
        "  - Return the final distributions and attention distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UazNlaCukV-N"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class PGNet(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, top_n = 3):\n",
        "        super().__init__()\n",
        "        self.top_n = top_n\n",
        "        self.embed_size = embed_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_IDX)\n",
        "\n",
        "        self.encoder = Encoder(embed_size, hidden_size)\n",
        "        self.decoder = AttentionDecoder(embed_size, hidden_size, vocab_size)\n",
        "\n",
        "        self.w_context = nn.Linear(hidden_size * 2, 1, bias=False)\n",
        "        self.w_hidden = nn.Linear(hidden_size, 1, bias=False)\n",
        "        self.w_input = nn.Linear(embed_size, 1, bias=True)\n",
        "\n",
        "\n",
        "    def forward(self, source_tensor, target_tensor, pad_mask, enc_len):\n",
        "\n",
        "        batch_size=source_tensor.size(0)\n",
        "        d_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_IDX)\n",
        "        \n",
        "        enc_emb = self.embedding(source_tensor)\n",
        "        encoder_outputs, (h, c) = self.encoder(enc_emb, enc_len)\n",
        "\n",
        "        final_dists = []\n",
        "        attn_dists = []\n",
        "\n",
        "        #TODO: mention max TARGET_LEN within range function below\n",
        "        if target_tensor is not None:\n",
        "            target_len = target_tensor.size(1)\n",
        "        else: \n",
        "            target_len = TARGET_LEN\n",
        "        for t in range(target_len):\n",
        "            input = self.embedding(d_input)\n",
        "            vocab_dist, attn_dist, context, h, c = self.decoder(input, h, c, encoder_outputs, pad_mask)\n",
        "\n",
        "            #TODO: apply the appropiate linear layers to\n",
        "                 # context vector, decoder hidden states and  decoder input\n",
        "            context_feature = self.w_context(context)\n",
        "            decoder_feature  = self.w_hidden(h)\n",
        "            input_feature = self.w_input(input)\n",
        "\n",
        "            #TODO: obtain generation features by adding up all features\n",
        "            gen_feat = context_feature + decoder_feature + input_feature.squeeze(-1)\n",
        "\n",
        "            #TODO: obtain generation probability by applying sigmoid to generation feature\n",
        "            p_gen = torch.sigmoid(gen_feat.squeeze())\n",
        "\n",
        "            #TODO: Calculate new vocab distribution by multiplying to p_gen\n",
        "            vocab_dist = vocab_dist * p_gen.unsqueeze(1)\n",
        "\n",
        "            #TODO: Calculate weighted attention distribution by\n",
        "                # multiplying attention distribution to (1-p_gen)\n",
        "            w_attn_dist = attn_dist * (1 - p_gen).unsqueeze(1)\n",
        "\n",
        "            #TODO:  add vocab_dist to w_attn_dist on the indexes of source_tensor\n",
        "            final_dist = vocab_dist.scatter_add(dim=-1, index=source_tensor, src=w_attn_dist)\n",
        "            final_dists.append(final_dist)\n",
        "            attn_dists.append(attn_dist)\n",
        "\n",
        "            #TODO:  If `target_tensor` is provided, use teacher forcing\n",
        "                  # by feeding the actual target token as the next input;\n",
        "                  # otherwise, use the predicted token from the current step.\n",
        "            if target_tensor is not None:\n",
        "                d_input = target_tensor[:, t:t+1]\n",
        "            else:\n",
        "                # Adding randomness to the selection of the next input\n",
        "                _, topi = torch.topk(final_dist, self.top_n , dim = -1)  \n",
        "                d_input = torch.empty(d_input.size(), dtype=torch.int32, device=device)\n",
        "                for i in range(topi.shape[0]):\n",
        "                    d_input[i] = topi[i, torch.randint(0, self.top_n, (1,))]\n",
        "\n",
        "        final_dists = torch.stack(final_dists, dim=1)\n",
        "        attn_dists = torch.stack(attn_dists, dim=1)\n",
        "\n",
        "        return final_dists, attn_dists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMTbpmQcGa5G"
      },
      "source": [
        "### Task 6. Training [ 1.5 points]\n",
        "\n",
        "*   Decide on model and training parameters\n",
        "*   Implement the training loop\n",
        "   *   Report training and validation loss for each epoch. You can refer to Equations 6 and 7 in the [ paper](https://arxiv.org/pdf/1704.04368.pdf#page=3) regarding loss calculation.\n",
        "   *   If validation loss is decreased, save the model.\n",
        "*   Initialize the model and train the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OW_cuhsaH_AM"
      },
      "outputs": [],
      "source": [
        "model = PGNet(len(joint_vocab), embed_size=128, hidden_size=256)\n",
        "#model.load_state_dict(torch.load('best_pgnet.pth'))\n",
        "model = model.to(device)\n",
        "criterion = nn.NLLLoss(ignore_index=PAD_IDX)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UYziTuGJH-rl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(8.0975, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(8.1348, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(8.2066, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(8.0441, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(8.1492, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(8.3777, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(7.8367, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(7.4861, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(7.4092, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(7.3479, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.9020, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.7174, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.5171, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.1269, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.0678, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.3821, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.5508, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.1273, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.3230, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.0596, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.9668, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.3247, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.0990, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.2788, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.1588, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.0069, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.1002, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.2450, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.2711, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.1896, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.0158, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.0491, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.1663, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.0290, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.0240, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.9287, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.7042, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.8827, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.1100, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.7379, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.7864, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.8334, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.1496, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.7527, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.0166, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.9960, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(6.0493, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.5659, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.8454, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.7173, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.7907, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.9965, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.7475, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.6247, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.6919, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.7095, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.5946, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.4902, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.4221, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.5350, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.4865, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.8459, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.4489, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.5149, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.5592, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.5100, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.4085, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.3859, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.4795, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.5426, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.3911, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.4784, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.6075, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.5059, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.3822, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.2278, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1494, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.5201, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.2637, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1996, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.2776, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1770, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.5519, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.3086, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.2100, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1299, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.5140, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.4580, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.4847, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.3368, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1152, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.4353, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.3587, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9989, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1345, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1565, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.2061, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.2978, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.3914, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.2201, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1075, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.3565, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.4070, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1709, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9192, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0005, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.3728, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0048, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1630, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9973, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.2437, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1362, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9457, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0848, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.2620, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1529, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9257, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1128, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0228, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1766, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1544, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.3598, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0337, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1293, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1237, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0069, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0527, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1727, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9064, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0513, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8613, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9518, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.2790, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0753, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8916, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7858, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0918, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1637, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0395, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9711, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1342, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8522, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9639, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0723, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7601, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0624, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8391, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1916, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6750, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1047, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9348, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.2588, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0285, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0369, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0404, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9702, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9223, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9143, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8946, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7014, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7914, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0668, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7423, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9250, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9051, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6830, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8533, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7968, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9028, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6239, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0180, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0288, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6727, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0942, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0388, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8841, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0716, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9493, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0438, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1468, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0242, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9844, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9037, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9519, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8514, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0825, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6478, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7726, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5752, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0421, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6722, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9321, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8180, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0673, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7601, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8358, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6801, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6220, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9782, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8256, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8610, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7286, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8345, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8357, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0427, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8378, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5626, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7762, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8392, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0571, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6811, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9964, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8550, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6450, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6873, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0107, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.0593, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6605, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1011, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7289, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8286, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6334, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8267, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7722, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9557, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8461, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5486, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5975, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.9638, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5701, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(5.1661, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "finished epoch\n",
            "Epoch 0: Training Loss: 5.376346906026204\n",
            "Epoch 0: Validation Loss: 3.8848843574523926\n",
            "Model saved.\n",
            "tensor(4.6051, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7440, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4587, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5913, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6589, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8502, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4652, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5126, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5811, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6844, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5659, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6858, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4306, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5444, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4090, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5216, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6443, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4343, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7652, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3286, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4030, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5035, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6219, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7155, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6094, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6605, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5830, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5708, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6807, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5724, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6706, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4951, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4221, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5206, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4120, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7173, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5310, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5833, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4434, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4800, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4857, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4828, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5592, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3821, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5362, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6269, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8486, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5454, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6130, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5221, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7269, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5894, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6818, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6044, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.8439, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5564, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4809, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5408, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4408, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7653, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6683, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4348, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5429, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4886, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4576, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4805, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7757, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3367, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4739, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7387, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5293, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7501, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4440, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4058, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5944, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4232, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6737, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7210, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7211, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4898, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5750, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5434, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5978, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3480, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5051, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4749, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6869, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5426, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6597, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4426, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5753, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7160, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4744, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6720, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6570, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3558, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4954, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5221, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4427, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2751, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6068, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5408, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2833, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5199, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4834, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5192, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6184, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4473, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4410, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4157, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3718, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6115, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4272, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3525, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4559, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4616, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2200, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5554, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7265, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3681, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5733, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5497, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7153, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4628, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1431, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4028, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6907, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5376, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2775, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3314, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3323, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1737, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5695, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3227, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1700, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6078, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6195, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3958, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3651, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4589, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2907, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2206, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6191, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4978, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4203, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3457, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7318, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5653, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3972, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7918, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4592, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5503, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6356, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3835, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3074, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4452, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5220, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5635, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2497, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5324, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3607, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5498, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3159, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3781, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5208, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4063, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4219, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4575, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3642, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4391, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2683, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3366, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3287, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4252, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6915, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5621, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5600, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4596, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2115, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4475, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3933, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1769, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3453, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3895, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4459, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2997, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6671, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2006, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3454, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1982, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3215, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2786, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2352, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3128, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3925, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3061, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5337, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3147, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5460, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3152, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4825, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1839, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4387, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6562, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5226, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4730, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3860, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2221, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3664, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1680, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2030, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.7383, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4631, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3815, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3023, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3527, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6519, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6091, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3979, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6051, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4077, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3341, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4302, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3635, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5349, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2738, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3430, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4982, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5681, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2308, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3521, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "finished epoch\n",
            "Epoch 1: Training Loss: 4.4863241307147135\n",
            "Epoch 1: Validation Loss: 3.478102684020996\n",
            "Model saved.\n",
            "tensor(4.0538, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3252, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9926, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0855, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2335, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3275, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1787, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2752, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3352, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0174, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2485, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1343, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4225, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1714, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1562, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1874, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1967, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1232, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9703, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1301, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9378, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1240, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1949, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9781, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2913, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0225, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2401, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2229, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2753, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1308, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1436, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2313, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2654, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9909, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9857, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0932, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0679, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3339, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2507, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1575, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2999, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1269, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3958, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2481, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9087, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0719, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9647, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9557, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2150, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4149, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2676, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0912, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2681, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2981, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2856, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1351, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2355, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2440, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3225, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1737, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0992, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2040, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0872, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2171, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0903, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2201, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1944, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1358, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1431, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2879, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0016, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1926, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3118, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0813, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1105, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2128, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1032, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0057, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1553, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9989, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0710, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1567, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1629, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2719, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1288, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1469, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2818, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2132, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1127, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1851, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2407, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4055, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2497, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1488, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4384, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2967, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2882, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2651, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1684, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9619, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1517, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0895, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1204, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3380, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1034, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3070, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2244, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2197, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0133, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0014, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2795, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3672, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1349, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4752, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0615, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2033, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0769, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1618, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0442, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2342, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0294, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2793, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2062, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1457, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1581, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2830, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1757, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3660, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0994, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3035, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9581, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2111, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1990, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3159, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8352, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0852, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0149, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2109, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2412, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3755, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2821, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1126, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4263, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2952, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0582, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2726, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2373, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4535, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2007, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3749, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2598, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1306, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1734, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1831, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.5262, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2081, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3373, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1855, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9529, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3289, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2827, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1198, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0727, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1511, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2006, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1075, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1206, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0420, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1838, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1515, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2325, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3542, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2549, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1273, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2679, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3006, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3185, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0850, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9875, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2202, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4013, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0388, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0378, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2777, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1882, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3395, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3796, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0256, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0346, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3171, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0478, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9733, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2358, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2770, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1227, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2624, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1718, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0438, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2596, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2614, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2779, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0368, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2815, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2512, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4292, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0293, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2627, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1435, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1730, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1700, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2743, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1606, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9521, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1598, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0469, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2035, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4287, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1260, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4332, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2623, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8367, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.3396, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9869, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.4319, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0938, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0413, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0292, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0505, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2631, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1288, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.6460, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "finished epoch\n",
            "Epoch 2: Training Loss: 4.185285213189724\n",
            "Epoch 2: Validation Loss: 3.3475892543792725\n",
            "Model saved.\n",
            "tensor(4.0655, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8800, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0162, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7268, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1325, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0186, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9146, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0671, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0481, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7410, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8939, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0605, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0041, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7655, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0998, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1571, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9257, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0868, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9156, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9701, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0313, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9504, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9861, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9333, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0914, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8821, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9210, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1400, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8762, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9629, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8885, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0012, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8917, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8627, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9803, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1133, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0054, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0395, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8433, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7363, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8401, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0768, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8856, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8051, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8013, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1441, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8756, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9216, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2797, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6080, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1539, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0307, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9184, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0214, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7562, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9967, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1335, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7723, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9773, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9923, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9536, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0644, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1800, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1335, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9231, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9843, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1273, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8715, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8076, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9097, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0182, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0663, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7077, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7837, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9797, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9513, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8876, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8487, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9399, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0768, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1400, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1176, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0309, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0403, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9269, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9487, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9294, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1075, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7831, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6484, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1515, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1921, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9728, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8842, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7882, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8644, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8293, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8600, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2557, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9553, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9693, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9887, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9455, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7542, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8719, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9413, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9763, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1669, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8532, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9407, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8040, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0217, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2384, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0270, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9557, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9560, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1018, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8293, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7443, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7886, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1819, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9656, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9282, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8544, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0960, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7342, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2017, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1213, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1088, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0104, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9777, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9033, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9698, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0430, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8158, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9311, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0301, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8018, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9051, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0854, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8306, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7690, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8645, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9180, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7306, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8960, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9256, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5785, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0177, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9149, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8575, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6231, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0289, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9668, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8797, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9065, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0098, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0478, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0050, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0695, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1084, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9167, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1350, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8755, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0370, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0973, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6969, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9040, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7177, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9654, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0757, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9149, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8157, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0069, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8379, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9505, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9968, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0492, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8810, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1254, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0799, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0504, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9043, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1438, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8506, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0972, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9980, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1422, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0484, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8268, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7735, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8235, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7663, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9115, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2360, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0995, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9975, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9290, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1558, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9060, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0821, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8797, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0545, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7605, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1443, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8588, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7181, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1038, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7107, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7561, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0475, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0292, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0698, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0287, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0467, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0887, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0564, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0657, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9813, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9851, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0353, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8900, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2764, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9183, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8430, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8743, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.1111, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0384, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8528, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6673, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8094, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "finished epoch\n",
            "Epoch 3: Training Loss: 3.9578813577627208\n",
            "Epoch 3: Validation Loss: 3.2174367904663086\n",
            "Model saved.\n",
            "tensor(3.7544, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6240, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7907, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7887, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6474, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5969, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7086, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7207, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6924, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8857, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7905, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5333, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7221, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7259, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8794, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7699, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7111, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6672, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7091, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6111, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5669, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7163, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0099, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5017, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7929, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9392, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8494, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6350, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6354, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7487, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7388, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8868, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5620, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7119, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5794, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7825, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5478, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7856, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6456, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9176, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9548, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8195, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7930, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8819, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4351, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6219, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7424, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7648, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8207, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5225, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8159, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7267, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5834, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6165, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4788, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5720, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8264, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8022, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8899, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5784, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7055, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8393, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7146, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7801, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5947, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6874, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8640, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7551, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6223, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7482, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9476, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7263, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8058, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6316, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7309, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7748, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9050, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6264, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8695, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9426, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7198, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7067, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5018, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6900, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9369, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9008, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8045, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9212, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6027, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6935, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6819, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7762, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8000, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7201, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5310, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9134, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5792, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8346, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7801, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5920, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5401, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7959, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6082, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7253, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5437, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9331, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8333, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7253, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6199, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6762, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4921, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9326, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9070, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7836, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6890, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8410, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5901, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6203, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6825, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5866, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9277, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5208, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0564, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5654, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8974, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6829, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7324, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6611, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7159, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6336, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8512, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9330, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6643, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6816, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5477, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9519, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6705, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9373, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5138, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5781, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6184, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5876, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6582, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9102, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8579, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6942, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5455, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6802, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7505, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6397, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8008, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5932, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6952, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9793, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6994, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9885, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0049, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7785, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9322, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7017, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7341, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0702, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7129, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8676, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5338, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7672, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7828, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7172, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8694, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8227, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8416, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8126, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8865, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9820, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9541, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8323, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9323, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9386, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7223, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8601, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7667, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8516, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6489, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7597, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8097, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7884, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8870, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5608, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8540, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7857, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8561, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6832, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7821, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7957, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7442, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7083, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7719, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9406, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6165, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8082, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8435, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7703, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4359, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7766, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6477, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7560, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0546, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0624, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7442, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0401, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6386, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8181, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9273, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6570, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9642, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9069, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7584, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7432, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8668, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6037, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5611, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7951, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0272, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6862, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5833, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7632, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7754, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8912, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.0149, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.9003, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(4.2996, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "finished epoch\n",
            "Epoch 4: Training Loss: 3.755083009794161\n",
            "Epoch 4: Validation Loss: 3.3544116020202637\n",
            "tensor(3.4971, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3449, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5056, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7094, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3724, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3413, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4032, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3373, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6055, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3667, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5857, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3796, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6988, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6880, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3135, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5180, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4759, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.2511, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7626, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5342, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4857, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6035, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3448, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5546, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5988, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4219, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5487, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3968, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4764, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4646, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6495, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4919, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3516, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3357, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4586, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6497, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3514, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6905, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6860, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5606, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5473, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4650, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.1704, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4177, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4851, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5464, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5878, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4470, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5172, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5415, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6117, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4554, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4258, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5463, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8290, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8413, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4449, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6329, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5440, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3980, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5389, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.2762, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4832, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3424, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6637, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7195, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4772, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3202, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4361, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6941, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6377, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4548, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6172, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4554, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3815, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7089, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5888, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4860, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4272, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5553, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5373, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3808, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5515, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4532, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4665, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4697, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5470, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4427, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6305, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6264, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6140, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5822, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4693, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3890, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6232, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5197, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6275, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5854, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3793, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6944, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3155, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3518, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7179, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3950, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6409, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5322, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4265, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3538, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5250, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5338, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5356, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3033, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5649, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6294, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3901, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5268, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5557, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5065, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5695, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5465, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.2901, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3209, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3976, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3392, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5846, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4649, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7821, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7792, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.7931, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3757, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5359, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5103, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4704, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4805, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5819, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6187, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4596, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.6994, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3383, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.5213, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.3924, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.8101, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "tensor(3.4363, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate loss_t for each time step and sum\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(torch\u001b[38;5;241m.\u001b[39mtranspose(torch\u001b[38;5;241m.\u001b[39mlog(final_dists), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), target_tensor)\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\Gordei\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torch\\_tensor.py:461\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    458\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[0;32m    459\u001b[0m     )\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Gordei\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torch\\_tensor_str.py:677\u001b[0m, in \u001b[0;36m_str\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[0;32m    676\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Gordei\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torch\\_tensor_str.py:597\u001b[0m, in \u001b[0;36m_str_intern\u001b[1;34m(inp, tensor_contents)\u001b[0m\n\u001b[0;32m    595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[0;32m    596\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 597\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[0;32m    600\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
            "File \u001b[1;32mc:\\Users\\Gordei\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torch\\_tensor_str.py:349\u001b[0m, in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[0;32m    347\u001b[0m     )\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 349\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
            "File \u001b[1;32mc:\\Users\\Gordei\\miniconda3\\envs\\nlp\\Lib\\site-packages\\torch\\_tensor_str.py:137\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_epochs = 10\n",
        "best_val_loss = torch.inf\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in semsum_dataloader_train:  # Assuming train_loader is defined\n",
        "        source_tensor, target_tensor, source_len, target_len = batch\n",
        "        source_tensor, target_tensor = source_tensor.to(device), target_tensor.to(device)\n",
        "        source_len, target_len = source_len.to(device), target_len.to(device)        \n",
        "             \n",
        "        optimizer.zero_grad()\n",
        "        pad_mask = (source_tensor == PAD_IDX)\n",
        "        final_dists, _ = model(source_tensor, target_tensor, pad_mask, source_len)\n",
        "\n",
        "        # Calculate loss_t for each time step and sum\n",
        "        loss = criterion(torch.transpose(torch.log(final_dists), -1, -2), target_tensor)\n",
        "        print(loss)\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    print('finished epoch')\n",
        "    avg_train_loss = total_loss / len(semsum_dataloader_train)\n",
        "    print(f'Epoch {epoch}: Training Loss: {avg_train_loss}')\n",
        "\n",
        "    # # Validation loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0\n",
        "        for batch in semsum_dataloader_valid:  \n",
        "            source_tensor, target_tensor, source_len, target_len = batch\n",
        "            source_tensor, target_tensor = source_tensor.to(device), target_tensor.to(device)\n",
        "            source_len, target_len = source_len.to(device), target_len.to(device)        \n",
        "                \n",
        "            optimizer.zero_grad()\n",
        "            pad_mask = (source_tensor == PAD_IDX)\n",
        "            final_dists, _ = model(source_tensor, target_tensor, pad_mask, source_len)\n",
        "            # Calculate loss_t for each time step and sum\n",
        "            loss = criterion(torch.transpose(torch.log(final_dists), -1, -2), target_tensor)\n",
        "            total_val_loss += loss\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(semsum_dataloader_valid)\n",
        "        print(f'Epoch {epoch}: Validation Loss: {avg_val_loss}')\n",
        "\n",
        "        # Save the model if validation loss has decreased\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), 'best_pgnet.pth')\n",
        "            print('Model saved.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRPu_jbIHUkQ"
      },
      "source": [
        "### Task 7. Inference [0.5 points]\n",
        "\n",
        "*   Create an inference function that gets text as input and returns the summary of the text based on the trained model.\n",
        "*   Pass at least 10 samples from the test set to the inference function\n",
        "*   Print the dialogues, human written summary, and generated summary\n",
        "*   Question: What are the common patterns among the generated summaries?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OjIGLQ-8g3EX"
      },
      "outputs": [],
      "source": [
        "model = PGNet(len(joint_vocab), embed_size=128, hidden_size=256)\n",
        "model.load_state_dict(torch.load('best_pgnet.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "total_loss = 0\n",
        "\n",
        "def inference(s_batch, top_n):\n",
        "    preprocessed_s_batch = [preprocessing(s) for s in s_batch] \n",
        "    source_batch = convert_to_ids(preprocessed_s_batch, joint_vocab, tokenizer)\n",
        "    \n",
        "    source_eos = []\n",
        "    for source in source_batch:\n",
        "        source = torch.cat([source, torch.tensor([EOS_IDX])], dim=0)\n",
        "        source_eos.append(source)\n",
        "    \n",
        "    source_len = torch.tensor([len(source) for source in source_eos])\n",
        "    source_padded = pad_sequence(source_eos, batch_first=True, padding_value=joint_vocab['<pad>'])\n",
        "    source_len, source_padded = source_len.to(device), source_padded.to(device)\n",
        "    \n",
        "    pad_mask = (source_padded == PAD_IDX)\n",
        "\n",
        "    # Adding randomness to selected tokens\n",
        "    final_dists, _ = model(source_padded, None, pad_mask, source_len)\n",
        "    _, topi = torch.topk(final_dists, top_n , dim=-1)\n",
        "    targets = torch.empty(final_dists.shape[0:2], dtype=torch.int32, device=device)\n",
        "    for i in range(topi.shape[0]):\n",
        "        for j in range(topi.shape[1]):\n",
        "            targets[i, j] = topi[i,j, torch.randint(0, top_n, (1,))]\n",
        "\n",
        "    sentences = []\n",
        "    for target in targets:\n",
        "        target_l = target.tolist()\n",
        "        for i in range(len(target_l)):\n",
        "            if target_l[i] == EOS_IDX:\n",
        "                target_l = target_l[0:i+1]\n",
        "                break\n",
        "        target_strs = joint_vocab.lookup_tokens(target_l)\n",
        "        sentences.append(' '.join(target_strs))\n",
        "    return sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---DIALOGUE---\n",
            "Christina: I need your help!\n",
            "Lee: What's wrong?\n",
            "Christina: Computer not working ;(\n",
            "Lee: What happened?\n",
            "Christina: I was working on my paper for Mr. Anderson and it suddenly turned off...\n",
            "Lee: W8. What paper?\n",
            "Christina: He gave us an essay on the most meaningful event in American history. It's due 2moro.\n",
            "Lee: Good to know, no sleep 2nite... But what about ur computer?\n",
            "Christina: I can't turn it back on and my whole paper and other stuff are on it!\n",
            "Lee: Did you try charging it first?\n",
            "Christina: Do you think I'm that stupid?\n",
            "Lee: No, but the cable could've disconnected and you didn't notice.\n",
            "Christina: Lemme check. BRB.\n",
            "Lee: OK.\n",
            "Christina: UR right! Silly me!\n",
            "Lee: See? E123!\n",
            "Christina: Thank you <3\n",
            "Lee: UR welcome.\n",
            "---TRUE SUMMARY---\n",
            "Christina thinks her computer broke, and she's not able to finish her essay on American History. Lee forgot about the essay. Christina follows Lee's advice, and finds out that the cable is disconnected.\n",
            "---MODEL SUMMARY---\n",
            "christina is working a essay . the most meaningful . . <eos>\n",
            "---DIALOGUE---\n",
            "Rob: hey, quick question - you done with GTAV?\n",
            "Tom: yeah, cool game :)\n",
            "Rob: so can I borrow it?\n",
            "Tom: sure\n",
            "Tom: but what do I get in exchange? :P\n",
            "Rob: my undying gratitude :)\n",
            "Rob: or the new Spiderman if you want ;)\n",
            "Tom: sounds great :)\n",
            "Rob: ok, I'll drop by your place later today - will give you a call\n",
            "Tom: ok\n",
            "---TRUE SUMMARY---\n",
            "Rob will drop by Tom's place later today, in order to borrow GTA V and lend Tom the new Spiderman game in exchange.\n",
            "---MODEL SUMMARY---\n",
            "tom and tom and borrow by new . exchange new . gtav new spiderman . <eos>\n",
            "---DIALOGUE---\n",
            "Clementine: i just got a terrible from my landlord :-(\n",
            "Clementine: it says that i have to leave the apartment next month :-( :-( :-(\n",
            "Victoria: why?!?!?!?!?!\n",
            "Clementine: she found someone who'll pay twice what i'm paying right now\n",
            "Victoria: can she do that legally?\n",
            "Clementine: i think she can :-/\n",
            "Clementine: i don't have a contract or anything\n",
            "Victoria: that's really unfair ༼ つ ◕_◕ ༽つ\n",
            "Clementine: i have to say i kinda understand why she's doing this to me\n",
            "Clementine: this is a great location\n",
            "Clementine: it's central, near the bus stop, restaurants, shops...\n",
            "Victoria: yes but she still can't kick you out so suddenly\n",
            "Victoria: you can't give up without a fight (ง ͠° ͟ل͜ ͡°)ง\n",
            "Clementine: lol i don't know\n",
            "Clementine: at this point i'm more worried of where i'll stay if i don't find an apartment soon\n",
            "Clementine: they're hard to come by in this city :-(\n",
            "Victoria: girl what are you taking about?\n",
            "Victoria: you can always stay with me!!!\n",
            "Clementine: ARE YOU SERIOUS?!?!?!!\n",
            "Victoria: of course!\n",
            "Clementine: THANKS GIRL!!!!!\n",
            "Clementine: i'm really really grateful\n",
            "Victoria: you're my best friend i'd never let you out in the cold\n",
            "Clementine: THANK YOU!!!!!!!!!!!!!!! <3\n",
            "---TRUE SUMMARY---\n",
            "Clementine got a notice from the landlord - she needs to leave the apartment next month. Victoria proposes she can stay with her.\n",
            "---MODEL SUMMARY---\n",
            "clementine and going the apartment . <eos>\n",
            "---DIALOGUE---\n",
            "Kate: We're about to leave\n",
            "Mary: Ok.\n",
            "Kate: We'll be at your place in 1 hour \n",
            "---TRUE SUMMARY---\n",
            "Kate will be at Mary's place in 1 hour. \n",
            "---MODEL SUMMARY---\n",
            "kate is going to leave cinema to mary and but he weekend is not to <eos>\n",
            "---DIALOGUE---\n",
            "Blair: Hey guys, have u heard about VR?\n",
            "Dale: Virtual reality? Sure.\n",
            "Tiffany: What's VR?\n",
            "Blair: VR stands for virtual reality. It's used in many fields. For example games.\n",
            "Dale: Knew you'd mention games first ;) \n",
            "Tiffany: So, except games, where can it be used?\n",
            "Dale: There are really numerous applications! For example, designing a building. U can do the maths, draw the lines and finally see the product or show it to your client!\n",
            "Blair: Or in medicine! For example, the doctors scan the faulty organ and then make w VR projection and decide what the best treatment or surgery is.\n",
            "Tiffany: Or for example, medicine students can learn by performing surgeries in virtual reality?\n",
            "Blair: Absolutely!\n",
            "Dale: The possibilities are countless! I've even heard it can be used for weather forecasting. Don't know how, though.\n",
            "Blair: And I've just read you will be able to fly to the moon with VR!\n",
            "Tiffany: Can't you do that now? Like in a game or something?\n",
            "Dale: SpaceX?\n",
            "Blair: Exactly!\n",
            "Tiffany: What's SpaceX?\n",
            "Dale: A company set up by Elon Musk.\n",
            "---TRUE SUMMARY---\n",
            "Blair and Dale explain to Tiffany what VR is and its numerous applications.\n",
            "---MODEL SUMMARY---\n",
            "dale is stands many fields . she used be her moon . the lines . the doctors organ <eos>\n",
            "---DIALOGUE---\n",
            "Arianna: Hello. I am sorry but I seem to have lost my confirmation email, and with it my reference number. Any chance you could resend this? \n",
            "Robert: Hello, no problem. Where did you book to go and under what name?\n",
            "Arianna: I have booked the 4.35 coach to Birmingham, under A. Banssa.\n",
            "Robert: Just a moment please. \n",
            "Robert: Please find attached a confirmation of your booking. You will need to show this to the driver when getting on the bus.\n",
            "Robert: <file_other>\n",
            "Arianna: That's great thank you\n",
            "Robert: No worries, enjoy your trip.  \n",
            "---TRUE SUMMARY---\n",
            "Arianna will need to show the booking confirmation to the driver.\n",
            "---MODEL SUMMARY---\n",
            "arianna and robert will meet a this weekend . <eos>\n",
            "---DIALOGUE---\n",
            "Erick: have you ever owned a dog?\n",
            "Doug: a couple, yes\n",
            "Doug: why do you ask?\n",
            "Erick: i'm thikigng of giving Max one for his birthday\n",
            "Erick: but i don't know if he's too young for that\n",
            "Doug: dogs can be a handful\n",
            "Doug: unlike cats, they also have to be walked typically once a day\n",
            "Erick: do you think Max is capable enough to take care of a dog?\n",
            "Doug: he is too young\n",
            "Doug: i would wait a year or two\n",
            "Erick: thanks for your advice\n",
            "Erick: we'll wait\n",
            "Doug: maybe you should get him one of those drones that all the kids like\n",
            "Erick: sure, lol ;-)\n",
            "---TRUE SUMMARY---\n",
            "Erick wanted to give Max a dog for his birthday. Doug talked him out of it claiming he is too young for such a responsibility.\n",
            "---MODEL SUMMARY---\n",
            "erick dog is a dog of but they s nt like what she is too handful of people . <eos>\n",
            "---DIALOGUE---\n",
            "Artur: Where do we send the project from Start Mangt?\n",
            "Essa: To Daniel\n",
            "Artur: Thanks. How about those presentations from the beginning? Also to him?\n",
            "Essa: Yea, we send it all together\n",
            "Przemek: As pptx?\n",
            "Artur: All has to be in PDF, I remember him saying that\n",
            "Daniel: Speaking of, I have only received 15 emails. From 51 :p\n",
            "Artem: Deadline?\n",
            "Daniel: Today...\n",
            "Artem: What time?\n",
            "Daniel: Let's say 23:59 :)\n",
            "Artem: Ok, thanks\n",
            "Bartek: There is no way I can send it today. When do you have to send it to the professor? Maybe we can get some extension?\n",
            "Daniel: I spoke to the professor, he said that I have to send it to him by Sunday. I guess I can accept all during the night, but no later, sorry guys. Btw, 28/51\n",
            "Maria: We still have a few minutes left :D\n",
            "Daniel: I know we have exams next week, but please guys, the final is tomorrow. I can't send what I have so far - 31/51\n",
            "Daniel: <file_other>. Please have a look, all student numbers marked in green are the ones I have received on my email\n",
            "Viktor: I sent you mine over a week ago, but it isn't green?\n",
            "Daniel: Found it, thanks for pointing that out. If someone isn't marked just send it to me again :)\n",
            "Aigerim: I sent mine as well - 79730\n",
            "Daniel: You were marked already...\n",
            "Aigerim: Ok, sorry, thought you marked only people that haven't submitted yet\n",
            "---TRUE SUMMARY---\n",
            "Daniel is collecting the presentations from Start Mangt to send them to the professor by Sunday. He has received 31 e-mails out of 51 so far. The deadline is tomorrow at 23:59.\n",
            "---MODEL SUMMARY---\n",
            "artur and going for a project . the project . <eos>\n",
            "---DIALOGUE---\n",
            "George: Kayle is saying\n",
            "George: That he wont be back until next week\n",
            "Kayla: You know exactly when \n",
            "Kayla: hes back? \n",
            "George: probably 13th xd\n",
            "George: I don't feel like him being back\n",
            "Kayla: I know haha\n",
            "Kayla: Never washes his dishes\n",
            "George: never cleans up after him \n",
            "Kayla: Not the worst roommate tho \n",
            "George: xd\n",
            "George: Defo better than Ava\n",
            "Kayla: That girl was\n",
            "Kayla: Just horrible\n",
            "George: Kayle buys us food sometimes tho\n",
            "Kayla: That is true\n",
            "Kayla: There are good things and bad things about him\n",
            "Kayla: His room is still a mess since he moved in\n",
            "Kayla: And its almost the end of the year haha\n",
            "George: Jesus\n",
            "---TRUE SUMMARY---\n",
            "George lets Kayla know what Kayle will be back probably on the 13th. George doesn't miss him as Kayle is a tough roommate, yet still he's better than Ava. \n",
            "---MODEL SUMMARY---\n",
            "george and george and nt , in the weekend . george is be back in in work . <eos>\n",
            "---DIALOGUE---\n",
            "Tom: I think we need to get a new washing machine\n",
            "Anna: Why do you think so?\n",
            "Tom:  This one is too loud, it makes a lot of noise during a spinning cycle\n",
            "Anna: I haven't noticed\n",
            "Tom: It started last week, gets louder every day.\n",
            "Anna: Do you think it’s something serious?\n",
            "Tom: Could be.\n",
            "Anna: Well, it’s quite old after all. Maybe you’re right, it’s time. \n",
            "Tom:  I heard that there are models that are very quiet\n",
            "Anna: Who cares about the noise. I think it's working fine\n",
            "Tom:  I don't like it. I think we should look around\n",
            "Anna: Washing machine costs a lot of money\n",
            "Tom:  I believe that the store near us has a special offer on appliances right now\n",
            "Anna: How do you know?\n",
            "Tom:  I saw a sign when I was passing the store on the way home\n",
            "Anna: Ok, we can go and look\n",
            "Tom:  I am so exciting. I can't wait\n",
            "Anna: Let's go there this weekend.\n",
            "Tom:  Great! I hope we will find something in a good price. \n",
            "---TRUE SUMMARY---\n",
            "Tom thinks it is necessary to buy a new washing machine because the old one makes a strange noise. Anna doesn't think it is necessary but she agrees to go to the shops to check the offers.\n",
            "---MODEL SUMMARY---\n",
            "tom store of the is loud and get new . louder . tom is started loud a special washing . the spinning . but he will nt a noise washing during <eos>\n"
          ]
        }
      ],
      "source": [
        "inds = torch.randint(0, len(dataset['train']), (10,)).tolist()\n",
        "# No randomness here works best it seems, so top_n=1\n",
        "generated_summaries = inference([dataset['train'][i]['dialogue'] for i in inds], top_n=1)\n",
        "for i, rndi in enumerate(inds):\n",
        "    print('---DIALOGUE---')\n",
        "    print(dataset['train'][rndi]['dialogue'])\n",
        "    print('---TRUE SUMMARY---')\n",
        "    print(dataset['train'][rndi]['summary'])\n",
        "    print('---MODEL SUMMARY---')\n",
        "    print(generated_summaries[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are some commont patterns between the generated summaries. First of all, they are not very good... They tend to have to many full stops, likely since it's a common character in the vocab. They also tend to repeat words, this is likely due to the fact that only the last word is fed into the encoder and the model tends to get into loops. They also tend to repeat names of people from the dialogue. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
